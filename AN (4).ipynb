{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2907d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import scipy\n",
    "import scipy.fft as fft\n",
    "import scipy.linalg as linalg\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26cbd76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535be3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "\n",
    "#Initialize matrix for possible pathways\n",
    "\n",
    "path_base = np.empty((4,15))\n",
    "\n",
    "#Get user input for key terms to pass to the LLM call\n",
    "\n",
    "relevant = List(input())\n",
    "\n",
    "#Source methods from the LLM call and substitute entries in path_base\n",
    "\n",
    "path = []\n",
    "for i in range(len(path_base)):\n",
    "    entry = np.random.choice(path_base[i])\n",
    "    path = path + entry\n",
    "\n",
    "\n",
    "\n",
    "#Derive from the LLM call and place into following dict\n",
    "listed_probs = {}\n",
    "#Initialize probabilities\n",
    "path = dict(path)\n",
    "for value in listed_probs.values():\n",
    "    path.value(value) = value\n",
    "\n",
    "\n",
    "\n",
    "class Math(nn.Module):\n",
    "    def __init__(self, transform: str):\n",
    "        obj_classified_dict = classified_dict\n",
    "        super().__init()\n",
    "        \n",
    "        self.obj_classified_dict = {}\n",
    "        for key, method_name in classified_dict.items():\n",
    "            method = getattr(self, method_name) \n",
    "            self.obj_classified_dict[key] = method\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for func in dir(Math):\n",
    "            entry = getattr(Math, func)\n",
    "            method_names += entry\n",
    "\n",
    "            selection = random.choice(method_names)\n",
    "        transform = self.obj_classified_dict[selection]\n",
    "        return transform(x)\n",
    "\n",
    "#Easiest way to bypass difficult gradient calculations is to store prior states of the data\n",
    "class nonMath(torch.Autograd.Function):\n",
    "\n",
    "    def forward_hook(i, o, instance):\n",
    "        priors += i\n",
    "\n",
    "    def forward(priors, ctx):\n",
    "        priors = []\n",
    "        ctx.save_for_backward(priors)\n",
    "       \n",
    "\n",
    "\n",
    "    def backward(ctx, p):\n",
    "        pass\n",
    "\n",
    "#Classify using requires_grad whether non_math or math\n",
    "classified_dict = {}\n",
    "for method in ref_methods: \n",
    "    output = method(data)\n",
    "    try:\n",
    "        output.backward()\n",
    "        method = Math()\n",
    "        classified_dict[f'{method}'] = 'math'\n",
    "        setattr(Math, method.__name__ , lambda cls, *args, method=method: method(*args))\n",
    "    except RuntimeError as e:\n",
    "        method = nonMath()\n",
    "        classified_dict[f'{method}'] = 'non-math'\n",
    "        \n",
    "\n",
    "\n",
    "#Class to dropout individual neurons in the overall matrix, not just the path (handled by the non_math class)\n",
    "class IndivDropout(nn.Module):\n",
    "    def __init__(self, epochs, path, prob, priors, loss):\n",
    "        super().__init()\n",
    "        epochs = self.epochs\n",
    "        path = self.path\n",
    "        prob = self.prob\n",
    "        self.priors = priors\n",
    "        self.loss = loss #Edit this this is incorrect (needs to be a return result to define loss)\n",
    "    \n",
    "    \n",
    "#Evaluate the current neuronal state for standard loss metrics in the case of a Math object, and a special prior\n",
    "\n",
    "    def eval_neuron_state(self,reg, prob, current_epoch, epochs, path, data, reg_entry, pred_u, ground_u):\n",
    "        #The following portion considers the loss accumulated from the entire path\n",
    "        data = data\n",
    "        dev = np.std(data)\n",
    "        iter_delta = []\n",
    "        iterations_before_switch = 0\n",
    "        epochs_from_start = current_epoch\n",
    "        epochs = epochs\n",
    "        state_x = []\n",
    "        if epochs_from_start < epochs * 0.1:\n",
    "            for i in range(epochs_from_start, epochs_from_start + ((epochs * 0.1) - 1)):\n",
    "                state_x += (np.quantile(output, i +1  / 10) - np.quantile(output, i / 10))\n",
    "                epochs_from_start += 1 \n",
    "\n",
    "        for i in range(len(state_x)):\n",
    "            iter_delta += state_x[i] - np.quantile(data, i / 10)\n",
    "\n",
    "        if any(iter_delta) > dev:\n",
    "            self.switch_neuron()\n",
    "        elif any(x > 0 for x in state_x):\n",
    "            self.switch_neuron()\n",
    "\n",
    "        relevance_loss = []\n",
    "        for method in path:\n",
    "            if method.__name__ not in relevant:\n",
    "                relevance_loss += np.std(pred_u)\n",
    "\n",
    "        #Now we define loss as a combination of specialized form of data loss and the newly defined relevance loss \n",
    "        tot_loss = torch.mean(((iter_delta)**2)  + (reg_entry * relevance_loss)**2)\n",
    "\n",
    "        #PROBLEM: How to differentiate the loss contributions of individual neurons -> Primitive Soln: Compute loss between different priors and examine \n",
    "        \n",
    "        for method in path:\n",
    "            if isinstance(method, nonMath):\n",
    "                priors_loss = torch.mean((handle6 - handle5)**2 + (handle8 - handle7)**2)\n",
    "        \n",
    "            if isinstance(method, Math):\n",
    "                m_priors_loss = torch.mean((handle4 - handle3)**2 + (handle2 - handle1)**2)\n",
    "\n",
    "            if priors_loss > m_priors_loss:\n",
    "                pass \n",
    "                #PROBLEM 6/17: How can we possibly get to a global minimum with this approach\n",
    "\n",
    "        return tot_loss, indiv_loss\n",
    "\n",
    "       \n",
    "\n",
    "    def reactivate_neuron(reg, prob, current_epoch, epochs, path, data):\n",
    "        epochs_from_start = current_epoch\n",
    "\n",
    "#This specially address to how to apply 'gradient descent' to nonMath objects in the sequences\n",
    "def switch_neuron(path):\n",
    "    #Executive action: Remove method from path\n",
    "    for method in path:\n",
    "            if isinstance(method, nonMath):\n",
    "                nm_methods = []\n",
    "                priors_loss = torch.mean((handle6 - handle5)**2 + (handle8 - handle7)**2)\n",
    "                nm_methods += method\n",
    "        \n",
    "            if isinstance(method, Math):\n",
    "                m_priors_loss = torch.mean((handle4 - handle3)**2 + (handle2 - handle1)**2)\n",
    "                nm_methods += method\n",
    "\n",
    "            if priors_loss > m_priors_loss:\n",
    "                path.pop(np.random.choice(nm_methods))\n",
    "                path += np.random.choice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec59f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting methods in path_base as objects of IndivDropout -> goal is to set as such and expand further subobjection to Math and nonMath (PROBLEM 6/17)\n",
    "\n",
    "for method in path:\n",
    "    method = IndivDropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1031aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL ARCHITECTURE: Combining CNN and RNN to pass time-based patterns over the data to recognize periodic conditions for normalization\n",
    "\n",
    "#Pre-hook for priors ahead of any nonMath transformations of the data\n",
    "\n",
    "def pre_hook_fn(module, input):\n",
    "    print(f\"[Pre-hook] Before {module.__class__.__name__}\")\n",
    "    print(f\"Input: {input}\")\n",
    "\n",
    "def forward_hook_fn(module, input, output):\n",
    "    print(f\"Inside {module.__class__.__name__}\")\n",
    "    print(f\"Input shape: {input[0].shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "\n",
    "#NOTE: Model needs to include a random call of a classmethod from the nonMath and Math classes, but also be able to find proper path through gradient descent\n",
    "\n",
    "class Model(nn.Module):\n",
    "    super().__init()\n",
    "    def __init__(self, in_feat, out_feat):\n",
    "        self.conv =  nn.Conv1d(in_channels=in_feat, out_channels=in_feat // 2, stride = 3)\n",
    "        self.rnn  = nn.RNN(in_feat, hidden_size = in_feat // 2)\n",
    "        self.Linear = nn.Linear(in_feat, out_feat)\n",
    "        self.nonMath1 = nonMath()\n",
    "        self.nonMath2 = nonMath()\n",
    "        self.Math1 = Math()\n",
    "        self.Math2 = Math()\n",
    "\n",
    "\n",
    "#NOTE: Not sustainable to keep design as two math methods and two non-math methods -> expand functionality to any combination from here\n",
    "\n",
    "    #Model design is done this way such that loss computations for individual priors can be done somewhat evenly\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.nonMath1(x)\n",
    "\n",
    "        #Register forward hook \n",
    "\n",
    "\n",
    "        x = self.conv(x)\n",
    "        x = self.Linear(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.rnn(x)\n",
    "\n",
    "        #Register pre-hook\n",
    "\n",
    "        x = self.Math1(x)\n",
    "\n",
    "        #Register forward hook \n",
    "\n",
    "        x = self.conv(x)\n",
    "        x = self.Linear(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.rnn(x)\n",
    "\n",
    "        #Register-pre-hook\n",
    "\n",
    "        x = self.Math2(x)\n",
    "\n",
    "        #Register forward hook \n",
    "\n",
    "        x = self.conv(x)\n",
    "        x = self.Linear(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.rnn(x)\n",
    "    \n",
    "        #Register another pre-hook\n",
    "\n",
    "        x = self.nonMath2(x)\n",
    "\n",
    "        #Register forward hook \n",
    "\n",
    "model = Model()\n",
    "\n",
    "handle1 = model.Math1.register_forward_pre_hook(pre_hook_fn)\n",
    "handle2 = model.Math1.register_forward_hook(forward_hook_fn)\n",
    "handle3 = model.Math2.register_forward_pre_hook(pre_hook_fn)\n",
    "handle4 = model.Math2.register_forward_hook(forward_hook_fn)\n",
    "handle5 = model.nonMath1.register_forward_pre_hook(pre_hook_fn)\n",
    "handle6 = model.nonMath1.register_forward_hook(forward_hook_fn)\n",
    "handle7 = model.nonMath2.register_forward_pre_hook(pre_hook_fn)\n",
    "handle8 = model.nonMath2.register_forward_hook(forward_hook_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aa8bba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
