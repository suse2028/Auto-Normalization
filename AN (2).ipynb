{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2907d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import scipy\n",
    "import scipy.fft as fft\n",
    "import scipy.linalg as linalg\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26cbd76",
   "metadata": {},
   "source": [
    "CURRENT PROBLEM TO SOLVE (6/14): Get into the specfics of how to encode 'gradient descent' here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535be3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Initialize matrix for possible pathways\n",
    "\n",
    "path_base = np.empty((4,15))\n",
    "\n",
    "#Source methods from the LLM call and substitute entries in path_base\n",
    "\n",
    "path = []\n",
    "for i in range(len(path_base)):\n",
    "    entry = np.random.choice(path_base[i])\n",
    "    path = path + entry\n",
    "\n",
    "\n",
    "#Derive from the LLM call and place into following dict\n",
    "listed_probs = {}\n",
    "#Initialize probabilities\n",
    "path = dict(path)\n",
    "for value in listed_probs.values():\n",
    "    path.value(value) = value\n",
    "\n",
    "\n",
    "\n",
    "class Math(nn.Module):\n",
    "    def __init__(self, transform: str):\n",
    "        obj_classified_dict = classified_dict\n",
    "        super().__init()\n",
    "        \n",
    "        self.obj_classified_dict = {}\n",
    "        for key, method_name in classified_dict.items():\n",
    "            method = getattr(self, method_name) \n",
    "            self.obj_classified_dict[key] = method\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for func in dir(Math):\n",
    "            entry = getattr(Math, func)\n",
    "            method_names += entry\n",
    "\n",
    "            selection = random.choice(method_names)\n",
    "        transform = self.obj_classified_dict[selection]\n",
    "        return transform(x)\n",
    "\n",
    "#Easiest way to bypass difficult gradient calculations is to store prior states of the data\n",
    "class nonMath(torch.Autograd.Function):\n",
    "\n",
    "    def forward_hook(i, o, instance):\n",
    "        priors += i\n",
    "\n",
    "    def forward(priors, ctx):\n",
    "        priors = []\n",
    "        ctx.save_for_backward(priors)\n",
    "       \n",
    "\n",
    "\n",
    "    def backward(ctx, p):\n",
    "        pass\n",
    "\n",
    "#Classify using requires_grad whether non_math or math\n",
    "classified_dict = {}\n",
    "for method in ref_methods: \n",
    "    output = method(data)\n",
    "    try:\n",
    "        output.backward()\n",
    "        method = Math()\n",
    "        classified_dict[f'{method}'] = 'math'\n",
    "        setattr(Math, method.__name__ , lambda cls, *args, method=method: method(*args))\n",
    "    except RuntimeError as e:\n",
    "        method = nonMath()\n",
    "        classified_dict[f'{method}'] = 'non-math'\n",
    "        \n",
    "\n",
    "\n",
    "#Class to dropout individual neurons in the overall matrix, not just the path (handled by the non_math class)\n",
    "class IndivDropout(nn.Module):\n",
    "    def __init__(self, epochs, path, prob):\n",
    "        super().__init()\n",
    "        epochs = self.epochs\n",
    "        path = self.path\n",
    "        prob = self.prob\n",
    "\n",
    "\n",
    "    def switch_neuron(self, path):\n",
    "        \n",
    "    \n",
    "#Evaluate the current neuronal state\n",
    "\n",
    "    def eval_neuron_state(self,reg, prob, current_epoch, epochs, path, data):\n",
    "        data = data\n",
    "        dev = np.std(data)\n",
    "        iter_delta = []\n",
    "        iterations_before_switch = 0\n",
    "        epochs_from_start = current_epoch\n",
    "        epochs = epochs\n",
    "        state_x = []\n",
    "        if epochs_from_start < epochs * 0.1:\n",
    "            for i in range(epochs_from_start, epochs_from_start + ((epochs * 0.1) - 1)):\n",
    "                state_x += (np.quantile(output, i +1  / 10) - np.quantile(output, i / 10))\n",
    "\n",
    "        for i in range(len(state_x)):\n",
    "            iter_delta += state_x[i] - np.quantile(data, i / 10)\n",
    "\n",
    "        if any(iter_delta) > dev:\n",
    "            self.switch_neuron()\n",
    "        elif any(x > 0 for x in state_x):\n",
    "            self.switch_neuron()\n",
    "        \n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "    def reactivate_neuron(reg, prob, current_epoch, epochs, path, data):\n",
    "        epochs_from_start = current_epoch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1031aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL ARCHITECTURE: Combining CNN and RNN to pass time-based patterns over the data to recognize periodic conditions for normalization\n",
    "\n",
    "#Pre-hook for priors ahead of any nonMath transformations of the data\n",
    "\n",
    "def pre_hook_fn(module, input):\n",
    "    print(f\"[Pre-hook] Before {module.__class__.__name__}\")\n",
    "    print(f\"Input: {input}\")\n",
    "\n",
    "#NOTE: Model needs to include a random call of a classmethod from the nonMath and Math classes, but also be able to find proper path through gradient descent\n",
    "\n",
    "class Model(nn.Module):\n",
    "    super().__init()\n",
    "    def __init__(self, in_feat, out_feat):\n",
    "        self.conv =  nn.Conv1d(in_channels=in_feat, out_channels=in_feat // 2, stride = 3)\n",
    "        self.rnn  = nn.RNN(in_feat, hidden_size = in_feat // 2)\n",
    "        self.Linear = nn.Linear(in_feat, out_feat)\n",
    "        self.nonMath = nonMath()\n",
    "        self.Math = Math()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.nonMath(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.Linear(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.rnn(x)\n",
    "\n",
    "        #Register pre-hook\n",
    "\n",
    "        x = self.nonMath(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.Linear(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.Linear(x)\n",
    "\n",
    "        x = self.Math(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.Linear(x)\n",
    "        x = self.rnn(x)\n",
    "    \n",
    "        #Register another pre-hook\n",
    "\n",
    "        x = self.nonMath(x)\n",
    "\n",
    "model = Model()\n",
    "\n",
    "handle = model.relu.register_forward_pre_hook(pre_hook_fn)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
